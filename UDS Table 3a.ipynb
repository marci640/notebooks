{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import count\n",
    "from pyspark.sql.functions import when\n",
    "import json\n",
    "from bunsen.stu3.bundles import load_from_directory, extract_entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Request - HAPI FHIR API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"http://hapi.fhir.org/baseR4/Patient?address-state=California\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store JSON responses in FHIR directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "while index <= 50:\n",
    "    response = requests.get(\"http://hapi.fhir.org/baseR4/Patient\")\n",
    "    json_response = response.json()\n",
    "    path = 'fhir/data' + str(index) + '.json'\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(json_response, f, ensure_ascii=False, indent=4)\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('uds').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load bundles from FHIR directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundles = load_from_directory(spark, 'fhir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = extract_entry(spark, bundles, 'patient').cache()\n",
    "# The extract_entry method returns a Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = patients.select('birthDate', 'gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Age & gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_age = df.withColumn('age', F.round( (F.datediff(F.current_date(), df.birthDate)) / 365) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered =df_with_age.filter((df_with_age.age < 95) & (df_with_age.age >= 25) & (df_with_age.gender != 'unknown')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map age to ageGroup values for UDS Table 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mapped = df_filtered.withColumn(\"ageGroup\", \n",
    "                        when(df_filtered.age.between(25, 29), \"Ages 25-29\")\n",
    "                       .when(df_filtered.age.between(30, 34), \"Ages 30-34\")\n",
    "                       .when(df_filtered.age.between(35, 39), \"Ages 35-39\")\n",
    "                       .when(df_filtered.age.between(40, 44), \"Ages 40-44\")\n",
    "                       .when(df_filtered.age.between(45, 49), \"Ages 45-49\")\n",
    "                       .when(df_filtered.age.between(50, 54), \"Ages 50-54\")\n",
    "                       .when(df_filtered.age.between(55, 59), \"Ages 55-59\")\n",
    "                       .when(df_filtered.age.between(60, 64), \"Ages 60-64\")\n",
    "                       .when(df_filtered.age.between(65, 69), \"Ages 65-69\")\n",
    "                       .when(df_filtered.age.between(70, 74), \"Ages 70-74\")\n",
    "                       .when(df_filtered.age.between(75, 79), \"Ages 75-79\")\n",
    "                       .when(df_filtered.age.between(80, 84), \"Ages 80-84\")\n",
    "                       .when(df_filtered.age >= 85, \"Age 85 and over\")\n",
    "                       .otherwise(df_filtered.age))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group by ageGroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = (df_mapped\n",
    "    .groupby(df_mapped.ageGroup)\n",
    "    .pivot(\"gender\")\n",
    "    .agg(count(\"birthDate\"))\n",
    "    ).sort('ageGroup').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----+\n",
      "|  ageGroup|female|male|\n",
      "+----------+------+----+\n",
      "|Ages 25-29|   120|   1|\n",
      "|Ages 30-34|     0| 150|\n",
      "|Ages 35-39|     0|  30|\n",
      "|Ages 40-44|     2|   0|\n",
      "|Ages 45-49|    30|   0|\n",
      "|Ages 65-69|    30|   0|\n",
      "+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_grouped.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
